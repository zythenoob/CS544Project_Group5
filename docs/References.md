## References

### Models:

[distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) 

[bert-base-uncased](https://huggingface.co/bert-base-uncased) 

[Knowledge distillation](https://en.wikipedia.org/wiki/Knowledge_distillation?oldformat=true)

[Introducing DistilBERT, a distilled version of BERT](https://medium.com/huggingface/distilbert-8cf3380435b5)

### Dataset

[arxiv_dataset](https://huggingface.co/datasets/arxiv_dataset)

[Kaggle arXiv Dataset Homepage](https://www.kaggle.com/datasets/Cornell-University/arxiv)

[arxiv dataset paper](https://arxiv.org/abs/1905.00075)

[arXiv Dataset Github Homepage](https://github.com/mattbierbaum/arxiv-public-datasets)

### Finetune a pretrained model

[Fine-tune a pretrained model](https://huggingface.co/docs/transformers/training)

[DistilBERT on GPU Tutorial: Classification Problem](https://www.kaggle.com/code/atulanandjha/distilbert-on-gpu-tutorial-classification-problem)

### Continual Learning

[A continual learning survey](https://arxiv.org/pdf/1909.08383.pdf)

[Three scenarios for continual learning](https://arxiv.org/pdf/1904.07734.pdf)

[[ZH]Three scenarios for continual learning](https://blog.csdn.net/weixin_44605171/article/details/125341304)

[Overcoming catastrophic forgetting in neural
networks](https://arxiv.org/pdf/1612.00796.pdf)
