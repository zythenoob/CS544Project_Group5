## References
---

### Models:

[distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) 

[bert-base-uncased](https://huggingface.co/bert-base-uncased) 

[Knowledge distillation](https://en.wikipedia.org/wiki/Knowledge_distillation?oldformat=true)

[Introducing DistilBERT, a distilled version of BERT](https://medium.com/huggingface/distilbert-8cf3380435b5)

### Dataset

[arxiv_dataset](https://huggingface.co/datasets/arxiv_dataset)

### Finetune a pretrained model

[Fine-tune a pretrained model](https://huggingface.co/docs/transformers/training)

[DistilBERT on GPU Tutorial: Classification Problem](https://www.kaggle.com/code/atulanandjha/distilbert-on-gpu-tutorial-classification-problem)

